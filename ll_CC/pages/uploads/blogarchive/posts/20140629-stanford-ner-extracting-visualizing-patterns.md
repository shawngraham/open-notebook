title: Stanford NER, extracting & visualizing patterns
link: http://electricarchaeology.ca/2014/06/29/stanford-ner-extracting-visualizing-patterns/
author: fhg1711
description: 
post_id: 2079
created: 2014/06/29 13:56:30
created_gmt: 2014/06/29 18:56:30
comment_status: open
post_name: stanford-ner-extracting-visualizing-patterns
status: publish
post_type: post

# Stanford NER, extracting & visualizing patterns

This is just a quick note while I'm thinking about this. I say 'visualizing' patterns, but there are of course many ways of doing that. Here, I'm just going quick'n'dirty into a network. Say you have the [diplomatic correspondence of the Republic of Texas](http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt), and you suspect that there might be interesting patterns in the places named over time. You can use the [Stanford Named Entity Recognition](http://nlp.stanford.edu/software/CRF-NER.shtml) package to extract locations. Then, using some regular expressions, you can transform that output into a network file. BUT - and this is important - it's a format that carries some baggage of its own. Anyway, first you'll want the Correspondence. Over at [The Macroscope, we've already written about how you can extract the patterns of correspondence between individuals](http://www.themacroscope.org/?page_id=521) using regex patterns. This doesn't need the Stanford NER because there is an index to that correspondence, and the regex grabs & parses that information for you. But there is no such index for locations named. So grab that document, and feed it into the NER as [Michelle Moravec instructs on her blog here](http://historyinthecity.blogspot.ca/2014/06/how-to-use-stanfords-ner-and-extract.html). In the  terminal window, as the classifier classifies Persons, Organizations, and Locations, you'll spot blank lines between batches of categorized items (_edit: there's a classifier that'll grab time too; that'd be quite handy to incorporate here - SG_). These blanks correspond to the blanks between the letters in the original document. Copy all of the terminal output into a new Notepad++ or Textwrangler document. We're going to trim away every line that isn't led by LOCATION: ` \n[^LOCATION].+` and replace with nothing. This will delete everything that doesn't have the location tag in front. Now, let's mark those blank lines as the start of a new letter. A thread on Stack Overflow suggests this regex to find those blank lines: ` ^\s*$ ` where: ^ is the beginning of string anchor $ is the end of string anchor \s is the whitespace character class * is zero-or-more repetition and we replace with the string new-letter. Now we want to get all of the locations for a single letter into a single line. Replace 'LOCATION' with a comma. This budges everything into a _single_ line, so we need to reintroduce line breaks, by replacing 'new-letter' with the new line character: ` find: (new-letter) replace \n(\1) ` I could've just replaced new-letter with a new-line, but I wanted to make sure that every new line did in fact start with new-letter. Now find and replace new-letter so that it's removed. You now have a document with the same number of lines as original letters in the original correspondence file. Now to turn it into a network file! Add the following information at the start of the file: DL n=721 format = nodelist1 labels embedded: data: DL will tell a network analysis program that we are dealing with UCINET's DL format. N equals the number of nodes. Format=nodelist1 says, 'this is a format where the first item on the line is connected to all the subsequent items on that line'. As a historian or archaeologist, you can see that there's a big assumption in that format. Is it justified? That's something to mull over. Gephi only accepts DL in format=edgelist1, that is, binary pairs. If that describes the relationship in your data, there's a lot of legwork involved in moving from nodelist1 to edgelist1, and I'm not covering that here. Let's imagine that, on historical grounds, nodelist1 accurately describes the relationship between locations mentioned in letters, that the first location mentioned is probably the place where the letter is being written from, or the most important place, or.... "labels embedded:" tells a network program that the labels themselves are being used as data points, and "data:" indicates that everything afterwards is the data. But how did we know how many nodes there were? You could tally up by hand; you could copy and paste your data )(back when each LOCATION was listed) into a spreadsheet and use its COUNT function to find uniques; I'm lazy and just bang any old number in there, and then save it with a .dl extension.  Then I open it using [a small program called Keyplayer](http://www.analytictech.com/keyplayer/keyplayer.htm). This isn't what the program is for, but it will give you an error message that tells you the _correct_ number of nodes! Put that number into your DL file, and try again. If you've got it right, Keyplayer won't do anything - its silence speaks volumes (you can then run an analysis in keyplayer. If your DL file is not formatted correctly, no results!). You now have a DL file that you can analyze in [Pajek](http://pajek.imfm.si/doku.php) or [UCINET](https://sites.google.com/site/ucinetsoftware/home). If you want to visualize in Gephi, you have to get it into a DL format that Gephi can use (edgelist) or else into .net format. Open your DL file in Pajek, and then save as Pajek format (which is .net). Then open in Gephi. (Alternatively, going back a step, you can open in Keyplayer, and then within Keyplayer, hit the 'visualize in Pajek' button, and you'll automatically get that transformation). (_edit: if you're on a Mac, you have to run Pajek or Ucinet with something like [Winebottler](http://winebottler.kronenberg.org/). Forgot to mention that)._ Ta da! [caption id="attachment_2082" align="alignleft" width="300"]![Locations mentioned in letters of the Republic of Texas](http://electricarchaeologist.files.wordpress.com/2014/06/texas-locations.png?w=300) Locations mentioned in letters of the Republic of Texas[/caption]